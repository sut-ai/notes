---
layout: content
metadata: notebooks_11_temporal_probability_models_metadata
colab: https://colab.research.google.com/github/sut-ai/notes/blob/master/notebooks/11_temporal_probability_models/index3.ipynb/index.ipynb
---

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Robot-Localization">Robot Localization<a class="anchor-link" href="#Robot-Localization">¶</a></h2><p>In robot localization, we know the map, but not the robot’s position. An example of observations would be vectors of range finder readings, this means our agent has a couple of sensors, each reporting the distance in a specific direction with an obstacle. State space and readings are typically continuous (works basically like a very fine grid) and so we cannot store $B(X)$. Due to this property of problem, particle filtering is a main technique.</p>
<p>So, we use many particles, uniformly distributed in the map. Then, after each iteration, we become reluctant to those of them that do not have probable readings. As a result, trusting that map would have been different to the eyes of our particles, we would end up with our particles centered at the real position.</p>
<p>The below depiction shows this perfectly. The red dots represent particles. Notice how the algorithm can't decide between two positions until entering a room.</p>
<p>What algorithm do you think would be better to drive the agent with, so that we can find and benefit from asymmetries in the map? (Think about random walks)</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="robot localization" src="/notes/assets/notebooks/11_temporal_probability_models/resource/robot-localization.gif"/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can even even go a step further, and forget about the map. This problem is called <strong>Simultaneous Localization And Mapping</strong> or <strong>SLAM</strong> for short. In this version of problem, we neither do know where the agent is, nor know what the map is. We have to find them both.</p>
<p>To solve this problem, we extend our states to also cover the map. For example, we can show our map with a matrix of 1s and 0s where every element is 1 if the map is blocked in the corresponding region on the map.</p>
<p>To solve this problem we use Kalman filtering and particle methods.</p>
<p>Notice how the robot starts with complete certainty about its position, and as the time goes on, it doubts if the position indeed is probable if it was a little bit away from its current position (like the readings would have been close to what they are now) and this leads to uncertainty even about the position. When the agent reachs a full cycle, it understands that it should be at the same position now, so its certainty about its position rises once again.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dynamic-Bayes-Net">Dynamic Bayes Net<a class="anchor-link" href="#Dynamic-Bayes-Net">¶</a></h2><p>Dynamic Bayesian Networks (<strong>DBN</strong>) extend standard Bayesian networks with the concept of time. This allows us to model time series or sequences. In fact they can model complex multivariate time series, which means we can model the relationships between multiple time series in the same model, and also different regimes of behavior, since time series often behave differently in different contexts.</p>
<p><img alt="dbn" src="/notes/assets/notebooks/11_temporal_probability_models/resource/dbn.png"/></p>
<h3 id="DBN-Particle-Filters">DBN Particle Filters<a class="anchor-link" href="#DBN-Particle-Filters">¶</a></h3><p>A particle is a complete sample for a time step. This is similar to reqgular filtering where we have to use sampling methods introduced earlier in the course instead of just a distribution.</p>
<p>Below are the steps we have to follow:</p>
<ul>
<li>Initialize</li>
</ul>
<p>Generate prior samples for the $t=1$ Bayes net. e.g. particle $G_1^a = (3,3) G_1^b = (5,3)$ for above image.</p>
<ul>
<li>Elapse time</li>
</ul>
<p>Sample a successor for each particle. e.g. successor $G_2^a = (2,3) G_2^b = (6,3)$</p>
<ul>
<li>Observe</li>
</ul>
<p>Weight each entire sample by the likelihood of the evidence conditioned on the sample.
Likelihood $p(E_1^a |G_1^a) \times p(E_1^b |G_1^b)$</p>
<ul>
<li>Resample</li>
</ul>
<p>Select prior samples (tuples of values) in proportion to their likelihood.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Most-Likely-Explanation">Most Likely Explanation<a class="anchor-link" href="#Most-Likely-Explanation">¶</a></h2><p><img alt="mle" src="/notes/assets/notebooks/11_temporal_probability_models/resource/mle.png"/></p>
<p>We are introducing a new query, we can ask our temporal model. The query statement is as follows:</p>
<pre><code>What is the most likely path of states that would have produced the current result.

</code></pre>
<p>Or more formally if our states are $X_i$ our observations are $E_i$, we want to find</p>
$$argmax_{x_{1:t}} P(x_{1:t}|e_{1:t})$$<p>But how can we answer this query?</p>
<p>First, let's define the <strong>state trellis</strong>.</p>
<p><img alt="trellis" src="/notes/assets/notebooks/11_temporal_probability_models/resource/trellis.png"/></p>
<p>State trellis is a directed weighted graph $G$ that its nodes are the states, and an arc between two states $u$, and $v$ represents a transition between these two states. The weight of this arc is defined by the probablity of this arc happening. More formally, assume we have a transition between $x_{t-1}$ and $x_t$. Then the weight of the arc between these two will be $P(x_{t}|x_{t-1}) \times P(e_t|x_t)$</p>
<p>Note that with this definition, each path is a sequence of states, and the product of weights in this path is the probability of this path, provided the evidence.</p>
<h3 id="Viterbi's-Algorithm">Viterbi's Algorithm<a class="anchor-link" href="#Viterbi's-Algorithm">¶</a></h3><p>Viterbi, uses dynamic programming model, to find the best path along the states. It first finds how probable a state at time $t-1$ is, and then reasons that the state at time $t$ relies solely on last state, and so having those probablities is enough to find the probability of new steps. Finally, the state that helps us find the most likely last state is it's parent.</p>
\begin{align*}
m_t[x_t] &amp;= max_{x_{1:t-1}} P(x_{1:t-1}, x_t, e_{1:t}) \\
&amp;= P(e_t|x_t)max_{x_{t-1}} P(x_t|x_{t-1})m_{t-1}[x_{t-1}]
\end{align*}$$p_t[x_t] = argmax_{x_{t-1}} P(x_t|x_{t-1})m_{t-1}[x_{t-1}]$$<h4 id="Example">Example<a class="anchor-link" href="#Example">¶</a></h4><p>Consider a village where all villagers are either healthy or have a fever and only the village doctor can determine whether each has a fever. The doctor diagnoses fever by asking patients how they feel. The villagers may only answer that they feel normal, dizzy, or cold.</p>
<p>The doctor believes that the health condition of his patients operates as a discrete Markov chain. There are two states, "Healthy" and "Fever", but the doctor cannot observe them directly; they are hidden from him. On each day, there is a certain chance that the patient will tell the doctor he is "normal", "cold", or "dizzy", depending on his health condition.</p>
<p>The observations (normal, cold, dizzy) along with a hidden state (healthy, fever) form a hidden Markov model (HMM).</p>
<p>In this piece of code, start_p represents the doctor's belief about which state the HMM is in when the patient first visits (all he knows is that the patient tends to be healthy). The particular probability distribution used here is not the equilibrium one, which is (given the transition probabilities) approximately <code>{'Healthy': 0.57, 'Fever': 0.43}</code>. The transition_p represents the change of the health condition in the underlying Markov chain. In this example, there is only a 30% chance that tomorrow the patient will have a fever if he is healthy today. The emit_p represents how likely each possible observation, normal, cold, or dizzy is given their underlying condition, healthy or fever. If the patient is healthy, there is a 50% chance that he feels normal; if he has a fever, there is a 60% chance that he feels dizzy.</p>
<p><img alt="health" src="/notes/assets/notebooks/11_temporal_probability_models/resource/health.png"/></p>
<p>The patient visits three days in a row and the doctor discovers that on the first day he feels normal, on the second day he feels cold, on the third day he feels dizzy. The doctor has a question: what is the most likely sequence of health conditions of the patient that would explain these observations?</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">obs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"normal"</span><span class="p">,</span> <span class="s2">"cold"</span><span class="p">,</span> <span class="s2">"dizzy"</span><span class="p">)</span>
<span class="n">states</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"Healthy"</span><span class="p">,</span> <span class="s2">"Fever"</span><span class="p">)</span>
<span class="n">start_p</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"Healthy"</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s2">"Fever"</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">}</span>
<span class="n">trans_p</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"Healthy"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"Healthy"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span> <span class="s2">"Fever"</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">},</span>
    <span class="s2">"Fever"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"Healthy"</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span> <span class="s2">"Fever"</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">},</span>
<span class="p">}</span>
<span class="n">emit_p</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"Healthy"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"normal"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">"cold"</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span> <span class="s2">"dizzy"</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">},</span>
    <span class="s2">"Fever"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"normal"</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s2">"cold"</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span> <span class="s2">"dizzy"</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">},</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">viterbi</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">start_p</span><span class="p">,</span> <span class="n">trans_p</span><span class="p">,</span> <span class="n">emit_p</span><span class="p">):</span>
    <span class="n">V</span> <span class="o">=</span> <span class="p">[{}]</span>
    <span class="k">for</span> <span class="n">st</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>
        <span class="n">V</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">st</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"prob"</span><span class="p">:</span> <span class="n">start_p</span><span class="p">[</span><span class="n">st</span><span class="p">]</span> <span class="o">*</span> <span class="n">emit_p</span><span class="p">[</span><span class="n">st</span><span class="p">][</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="s2">"prev"</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
    <span class="c1"># Run Viterbi when t &gt; 0</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">obs</span><span class="p">)):</span>
        <span class="n">V</span><span class="o">.</span><span class="n">append</span><span class="p">({})</span>
        <span class="k">for</span> <span class="n">st</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>
            <span class="n">max_tr_prob</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="s2">"prob"</span><span class="p">]</span> <span class="o">*</span> <span class="n">trans_p</span><span class="p">[</span><span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="n">st</span><span class="p">]</span>
            <span class="n">prev_st_selected</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">prev_st</span> <span class="ow">in</span> <span class="n">states</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
                <span class="n">tr_prob</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="n">prev_st</span><span class="p">][</span><span class="s2">"prob"</span><span class="p">]</span> <span class="o">*</span> <span class="n">trans_p</span><span class="p">[</span><span class="n">prev_st</span><span class="p">][</span><span class="n">st</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">tr_prob</span> <span class="o">&gt;</span> <span class="n">max_tr_prob</span><span class="p">:</span>
                    <span class="n">max_tr_prob</span> <span class="o">=</span> <span class="n">tr_prob</span>
                    <span class="n">prev_st_selected</span> <span class="o">=</span> <span class="n">prev_st</span>

            <span class="n">max_prob</span> <span class="o">=</span> <span class="n">max_tr_prob</span> <span class="o">*</span> <span class="n">emit_p</span><span class="p">[</span><span class="n">st</span><span class="p">][</span><span class="n">obs</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span>
            <span class="n">V</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">st</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"prob"</span><span class="p">:</span> <span class="n">max_prob</span><span class="p">,</span> <span class="s2">"prev"</span><span class="p">:</span> <span class="n">prev_st_selected</span><span class="p">}</span>

    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">dptable</span><span class="p">(</span><span class="n">V</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

    <span class="n">opt</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">max_prob</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">best_st</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Get most probable state and its backtrack</span>
    <span class="k">for</span> <span class="n">st</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">V</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="s2">"prob"</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_prob</span><span class="p">:</span>
            <span class="n">max_prob</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"prob"</span><span class="p">]</span>
            <span class="n">best_st</span> <span class="o">=</span> <span class="n">st</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_st</span><span class="p">)</span>
    <span class="n">previous</span> <span class="o">=</span> <span class="n">best_st</span>

    <span class="c1"># Follow the backtrack till the first observation</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">V</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">V</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">previous</span><span class="p">][</span><span class="s2">"prev"</span><span class="p">])</span>
        <span class="n">previous</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">previous</span><span class="p">][</span><span class="s2">"prev"</span><span class="p">]</span>

    <span class="nb">print</span> <span class="p">(</span><span class="s2">"The steps of states are "</span> <span class="o">+</span> <span class="s2">" "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">opt</span><span class="p">)</span> <span class="o">+</span> <span class="s2">" with highest probability of </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="n">max_prob</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">dptable</span><span class="p">(</span><span class="n">V</span><span class="p">):</span>
    <span class="c1"># Print a table of steps from dictionary</span>
    <span class="k">yield</span> <span class="s2">" "</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="s2">"     "</span><span class="o">.</span><span class="n">join</span><span class="p">((</span><span class="s2">"</span><span class="si">%3d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">V</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">V</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">yield</span> <span class="s2">"</span><span class="si">%.7s</span><span class="s2">: "</span> <span class="o">%</span> <span class="n">state</span> <span class="o">+</span> <span class="s2">" "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">"</span><span class="si">%.7s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="s2">"</span><span class="si">%lf</span><span class="s2">"</span> <span class="o">%</span> <span class="n">v</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="s2">"prob"</span><span class="p">])</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">V</span><span class="p">)</span>

<span class="n">viterbi</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">start_p</span><span class="p">,</span> <span class="n">trans_p</span><span class="p">,</span> <span class="n">emit_p</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>       0       1       2
Healthy: 0.30000 0.08400 0.00588
Fever: 0.04000 0.02700 0.01512
The steps of states are Healthy Healthy Fever with highest probability of 0.01512
</pre>
</div>
</div>
</div>
</div>
</div>
