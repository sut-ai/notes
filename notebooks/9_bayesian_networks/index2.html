---
layout: content
metadata: notebooks_9_bayesian_networks_metadata
colab: https://colab.research.google.com/github/sut-ai/notes/blob/master/notebooks/9_bayesian_networks/index.ipynb
---

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Bayesian-Network:-Representation"><strong>Bayesian Network: Representation</strong><a class="anchor-link" href="#Bayesian-Network:-Representation">¶</a></h1><h2 id="Table-of-Content"><strong>Table of Content</strong><a class="anchor-link" href="#Table-of-Content">¶</a></h2><ul>
<li><a href="#Bayesian-Network-Representation">Bayesian Network: Representation</a><ul>
<li><a href="#Table-Of-Content">Table of Content</a></li>
<li><a href="#Bayes'-Net-Examples">Bayes' Net Examples</a></li>
<li><a href="#Bayes'-Net-Semantics">Bayes' Net Semantics</a><ul>
<li><a href="#Example">Example</a></li>
<li><a href="#Joint-Dist.-Validation">Joint Dist. Validation</a></li>
<li><a href="#What-Cond.-Independencies-could-be-implied?">What Cond. Independencies could be implied?</a><ul>
<li><a href="#Proof">Proof</a></li>
</ul>
</li>
<li><a href="#Bayes'-Net-Probability-Examples">Bayes' Net Probability Examples</a></li>
</ul>
</li>
<li><a href="#Bayes'-Net-Size-Analysis">Bayes' Net Size Analysis</a></li>
<li><a href="#Does-Bayes'-Net-Necessarily-Represent-Causal-Relations?">Does Bayes' Net Necessarily Represent Causal Relations?</a></li>
<li><a href="#Conditional-Independencies-in-Bayes'-Net">Conditional Independencies in Bayes' Net</a></li>
</ul>
</li>
</ul>
<h2 id="Bayes'-Net-Examples"><strong>Bayes' Net Examples</strong><a class="anchor-link" href="#Bayes'-Net-Examples">¶</a></h2><p>These are some examples of how uncertain environment variables can be modeled as Bayesian Network. Try to justify relations between those variable to get a better grasp of how BN works.</p>
<h3 id="Coin-Flips-1"><strong>Coin Flips</strong> 1<a class="anchor-link" href="#Coin-Flips-1">¶</a></h3><p>Problem of tossing n independent coins is an example of modeling a probabilistic environment. No interaction between coins results in absolute independency.</p>
<figure>
<img alt="drawing" src="/notes/assets/./images_p2/coin_flip_example.png" width="400"/>
<center>
<figcaption>BN for independent coin flip of n coins</figcaption>
</center>
</figure><h3 id="Traffic-1"><strong>Traffic</strong> 1<a class="anchor-link" href="#Traffic-1">¶</a></h3><ul>
<li><p>Variables:</p>
<ul>
<li>R: It rains</li>
<li>T: There is a traffic on the road.</li>
</ul>
</li>
<li><p>Two models can be considered for such a problem:
<br/>
<img alt="drawing" src="/notes/assets/./images_p2/rain_traffic_1.png" width="200"/>
<br/>
An agent using dependent model usually is more realistic.</p>
</li>
</ul>
<h3 id="Second-Traffic-1"><strong>Second Traffic</strong> 1<a class="anchor-link" href="#Second-Traffic-1">¶</a></h3><p><img alt="drawing" src="/notes/assets/./images_p2/traffic_II_example.png" width="500"/></p>
<ul>
<li><p>Variables:</p>
<ul>
<li>T: Traffic.</li>
<li>R: It rains.</li>
<li>L: Low air pressure.</li>
<li>D: Stadium roof drips.</li>
<li>B: There is a ballgame.</li>
<li>C: There is a cavity in <em>Russell</em> 's teeth (name of the green monster).</li>
</ul>
</li>
<li><p>Model: 
  Low pressure might cause rain (R is dependent to L) and rain might cause traffic. A ballgame being held also might cause traffic and rain causes drips from stadium roof. <em>Russell</em>'s tooth cavity  is independent of other variables, so BN of this environment is presented in the following figure:</p>
<p><img alt="drawing" src="/notes/assets/./images_p2/second_traffic_1.png" width="300"/></p>
<p>In this environment rain can cause ballgame be canceled, so B is slightly dependent on R which is shown by blue arrow from R to B, but to keep our model as simple as possible, we tend to not include this relation in our BN. This results in ignoring some information from problem's environment, So we need to keep a balance between model simplicity and information loss.</p>
</li>
</ul>
<h3 id="Alarm-Network-1"><strong>Alarm Network</strong> 1<a class="anchor-link" href="#Alarm-Network-1">¶</a></h3><p>Previousy, we were interoduced to this problem to get familiar with probabilistic environment and problem scenarios. Here we present its varaibles and related Bayesian Network:</p>
<ul>
<li>Variables:<ul>
<li>B: Burglary</li>
<li>A: Alarm goes off</li>
<li>M: Marry calls</li>
<li>J: John calls</li>
<li>E: Earthquake</li>
</ul>
</li>
<li>Representation: <br/>
<img alt="drawing" src="/notes/assets/./images_p2/alarm_network_1.png" width="180"/></li>
</ul>
<h2 id="Bayes'-Net-Semantics"><strong>Bayes' Net Semantics</strong><a class="anchor-link" href="#Bayes'-Net-Semantics">¶</a></h2><p>BN is a directed acyclic graph in which every node refers to the probability of a random variable X conditioned to its parents. Here 
$$P(X | A_1, A_2, A_3, .., A_n)$$
means probability distribution of any x conditioned to every possible combination of its parents (A<sub>1</sub>, A<sub>2</sub>, A<sub>3</sub>, ....A<sub>n</sub>).  All these probability combinations are gathered in a table called conditional probability table (CPT). The following figure shows an example of CPT:</p>
<p><img alt="drawing" src="/notes/assets/./images_p2/bn_semantics.png" width="200"/></p>
<p>So every BN consists of <strong>Topology</strong>(graph) and <strong>Local Conditional Probabilities</strong>.</p>
<p>Bayes' Net <em>implicitly</em> encodes joint distribution. Joint distribution of random variables included in a BN is calculated using the product of all local conditional distributions. The following equation shows explicit formula:</p>
$$P(x_1,x_2,x_3,..,x_n) = \prod_{i=1}^{n}P(x_i|parents(X_i))$$<p>where x<sub>i</sub> is an observation of X<sub>i</sub>.</p>
<h3 id="Example"><strong>Example</strong><a class="anchor-link" href="#Example">¶</a></h3><p>Here is an example of how a joint distribution is embedded in a BN:</p>
<p><img alt="drawing" src="/notes/assets/./images_p2/joint_example.png" width="400"/></p>
<p>To compute P(+cavity, +catch, -toothache):
$$P(+cavity, +catch, -toothache) = \\P(+cavity|parents(Cavity))  \times P(+catch|parents(Catch)) \\\times P(+toothache|parents(Toothache)) \\
= P(+cavity) \times P(+catch|+cavity) \times P(-toothache|+cavity)$$</p>
<h3 id="Joint-Dist.-Validation"><strong>Joint Dist. Validation</strong><a class="anchor-link" href="#Joint-Dist.-Validation">¶</a></h3><p>So far we presented a joint distribution formula using BN, but we have not proved that it is a valid probability distribution.
There are 2 conditions for every joint distribution:
$$
\begin{cases}
    P(x_1,x_2,x_3,..,x_n) \ge 0 \qquad for \quad every \quad X_i\\
    \sum_{X_1, X_2, ..,X_n}  P(x_1,x_2,x_3,..,x_n) = 1  
\end{cases}
$$</p>
<p>The first condition is directly concluded from the definition. For the second condition, we can replace joint destribution with the defined formula. If we sort X<sub>i</sub>s as topological order, we have:</p>
$$
\sum_{X_1, X_2, ..,X_n}  P(x_1,x_2,x_3,..,x_n) = \sum_{X_1, X_2, ..,X_n} \prod_{j \in \{1,..n\}} P(x_i|parents(X_i)) \\
= \sum_{X_1} \sum_{X_2} \quad .. \prod_{j \in \{1,..n-1\}} P(x_i|parents(X_i)) \sum_{X_n} P(x_n|parents(X_n)) \\
= \sum_{X_1} \sum_{X_2} \quad .. \prod_{j \in \{1,..n-1\}} P(x_i|parents(X_i)) 
= 1
$$<p>So this joint distribution is valid.</p>
<h3 id="What-Cond.-Independencies-could-be-implied?"><strong>What Cond. Independencies could be implied?</strong><a class="anchor-link" href="#What-Cond.-Independencies-could-be-implied?">¶</a></h3><p>Using chain rule we already know:
$$
P(x_1,x_2,x_3,..,x_n) = \prod_{i=1}^{n}P(x_i|x_1,x_2,x_3,..,x_{i-1})
$$
Now assume this equation hold:
$$
P(x_i|x_1,x_2,x_3,..,x_{i-1}) = P(x_i|parents(X_i))
$$
This results in:
$$
\Rightarrow P(x_1,x_2,x_3,..,x_n) = \prod_{i=1}^{n}P(x_i|parents(X_i))
$$
These equations show that there are some independency assumptions embedded in a BN; every node conditioned to its parents is independent of non-parent nodes that come before it, according to topological order - called <em>predecessors</em> for short.</p>
<p>Now does the reverse procedure also hold? In other way does 
$$
P(x_1,x_2,x_3,..,x_n) = \prod_{i=1}^{n}P(x_i|parents(X_i))
$$
conclude this:
$$
P(x_i|x_1,x_2,x_3,..,x_{i-1}) = P(x_i|parents(X_i))\qquad ?
$$</p>
<h4 id="Proof"><strong>Proof</strong><a class="anchor-link" href="#Proof">¶</a></h4>$$
P(x_i|x_1,x_2,x_3,..,x_{i-1}) = \frac{P(x_1,x_2,x_3,..,x_i)}{P(x_1,x_2,x_3,..,x_{i-1})} 
$$<p>We already know that:
$$
P(x_1,x_2,x_3,..,x_{i}) = \sum_{X_{i+1}, X_{i+2}, ..,X_n}  P(x_1,x_2,x_3,..,x_n) \\
= \sum_{X_{i+1}, X_{i+2}, ..,X_n} \prod_{j=1}^{n}P(x_j|parents(X_j))
$$
Considering topological order we have:
$$
P(x_1,x_2,x_3,..,x_{i}) 
=  \prod_{j=1}^{i}P(x_j|parents(X_j))
$$
$$
\Rightarrow \frac{P(x_1,x_2,x_3,..,x_i)}{P(x_1,x_2,x_3,..,x_{i-1})}  = \frac{\prod_{j=1}^{i}P(x_j|parents(X_j))}{\prod_{j=1}^{i-1}P(x_j|parents(X_j))} = P(x_i|parents(X_i))
$$
$$
\Rightarrow P(x_i|x_1,x_2,x_3,..,x_{i-1}) = P(x_i|parents(X_i)) \quad \square .
$$</p>
<h3 id="Bayes'-Net-Probability-Examples"><strong>Bayes' Net Probability Examples</strong><a class="anchor-link" href="#Bayes'-Net-Probability-Examples">¶</a></h3><h4 id="Coin-Flips-2"><strong>Coin Flips</strong> 2<a class="anchor-link" href="#Coin-Flips-2">¶</a></h4><p><img alt="drawing" src="/notes/assets/./images_p2/coin_flip_2.png"/><br/></p>
<h4 id="Traffic-2"><strong>Traffic</strong> 2<a class="anchor-link" href="#Traffic-2">¶</a></h4><p><img alt="drawing" src="/notes/assets/./images_p2/traffic_2.png" width="400"/><br/></p>
<h4 id="Reverse-Traffic-1"><strong>Reverse Traffic</strong> 1<a class="anchor-link" href="#Reverse-Traffic-1">¶</a></h4><p><img alt="drawing" src="/notes/assets/./images_p2/reverse_traffic_1.jpg" width="400"/><br/></p>
<h4 id="Alarm-Network-2"><strong>Alarm Network</strong> 2<a class="anchor-link" href="#Alarm-Network-2">¶</a></h4><p><img alt="drawing" src="/notes/assets/./images_p2/alarm_network_2.png" width="800"/></p>
<h2 id="Bayes'-Net-Size-Analysis"><strong>Bayes' Net Size Analysis</strong><a class="anchor-link" href="#Bayes'-Net-Size-Analysis">¶</a></h2><p>As shown the <a href="#Alarm-Network-2">Alarm Network</a> example every node holds a CPT of 2<sup>number of parents</sup> probabilities. This makes BN a very more efficient method to present joint probability distribution.
$$
\sum_{any \ node \ x} 2^{\ number \ of \ parents \ of \ x} \le 2^n
$$</p>
<h2 id="Does-Bayes'-Net-Necessarily-Represent-Causal-Relations?">Does Bayes' Net Necessarily Represent Causal Relations?<a class="anchor-link" href="#Does-Bayes'-Net-Necessarily-Represent-Causal-Relations?">¶</a></h2><p>When BN reflects true causal patterns:</p>
<ul>
<li>nodes have fewer parents and it makes the BN simpler, </li>
<li>it's easier to think about what the BN mean, </li>
<li>and the interpretation from it is very easier.</li>
</ul>
<p>Assume we have 3 random variables:</p>
<ul>
<li>R: It rains.</li>
<li>T: There is traffic.</li>
<li>D: Roof drips.</li>
</ul>
<p>Its BN would be like this:</p>
<p><img alt="drawing" src="/notes/assets/./images_p2/causal_relation_p1.png"/><br/></p>
<p>Now if our agent is not able to know about whether it's raining or not, It will have only 2 random variables (T, D) and the BN will be one of these two(since T and D are not independent):</p>
<p><img alt="drawing" src="/notes/assets/./images_p2/causal_relation_p2.png"/><br/></p>
<p>So there might be an arrow from T to D or back from D to T, when there is no causal relation between them. In conclusion, BN do not necessarily reflect a causal pattern.</p>
<h3 id="What-Does-Bayes'-Net-Really-Represent?">What Does Bayes' Net Really Represent?<a class="anchor-link" href="#What-Does-Bayes'-Net-Really-Represent?">¶</a></h3><p>BN topology may happen to encode causal structure, but what it really encodes is conditional independence (Local Cond. Indep.)</p>
<h2 id="Conditional-Independencies-in-Bayes'-Net">Conditional Independencies in Bayes' Net<a class="anchor-link" href="#Conditional-Independencies-in-Bayes'-Net">¶</a></h2><p><img alt="drawing" src="/notes/assets/./images_p2/cond_indep_intro.png" width="400"/><br/></p>
<p>In this BN we already know these relations:
$$
\begin{cases}
    W \mkern10mu \rlap{\_}|| \mkern10mu X \mkern10mu | \mkern10mu Z \\
    W \mkern10mu \rlap{\_}|| \mkern10mu Y \mkern10mu | \mkern10mu Z
\end{cases}
$$
is the following independency concluded?
$$
W \mkern10mu \rlap{\_}|| \mkern10mu X \mkern10mu | \mkern10mu Y
$$
We'll prove P(W|X,Y) is constant with respect to X.
$$
P(W|X,Y) = \sum_Z P(W,Z|X,Y) = \sum_Z \frac{P(W,Z,X,Y)}{P(X,Y)} \\
= \sum_Z \frac{P(X)P(Y|X)P(Z|Y)P(W|Z)}{P(X)P(Y|X)} = \sum_Z P(Z|Y)P(W|Z) \\ \Rightarrow const.\ w.r.t.\ X
$$</p>
<p>So here is a cond. independency that can be concluded from BN. Is there any algorithm that can exploit all possible cond. independencies?</p>
</div>
</div>
</div>
