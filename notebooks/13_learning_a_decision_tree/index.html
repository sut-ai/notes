---
layout: content
metadata: notebooks_13_learning_a_decision_tree_metadata
colab: https://colab.research.google.com/github/sut-ai/notes/blob/master/notebooks/13_learning_a_decision_tree//index.ipynb
---

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div style="line-height:300%;">
<font face="XB Zar" size="5">
<div align="center">
<font face="IranNastaliq" size="30">
<p></p>
<p></p>
Decision Tree
                <p></p>
</font>
<font color="#FF7500">
Sharif University Of technology - Computer Engineering Depatment
            </font>
<p></p>
<font color="blue">
Artifical Intelligence - Dr. MH Rohban
            </font>
<br/>
<br/>
Spring 2021
        </div>
<hr/>
<font color="red" size="6">
<br/>
<div align="center">
Decision Tree And Its Application In Classification
                <br/>
</div>
</font>
<div align="center">
<br/>Writers Team<br/>
<font color="gray" size="6">
            Amirsadra Abdollahi<br/>
            Ashkan Khademian<br/>
            Amirmohammad Isazadeh<br/>
</font>
</div>
<hr/>
<style scoped="" type="text/css">
        p{
        border: 1px solid #a2a9b1;background-color: #f8f9fa;display: inline-block;
        };
        </style>
<div>
<h3>Contents</h3>
<ul style="margin-right: 0;">
<li>
<a href="#sec_whatisR">
                        What's Decision Tree
                    </a>
</li>
<li>
<a href="#sec_installR">
                        Decision Tree Examples
                    </a>
</li>
<li>
<a href="#sec_R_packages">
                      Building A Decision Tree
                    </a>
</li>
<li>
<a href="#sec_R_Intro">
                        Entropy And Information Gain
                    </a>
</li>
<li>
<a href="#sec_statFunc">
                        Learning Decision Tree
                    </a>
</li>
<li>
<a href="#sec_R_DSs">
                        Overfitting in Decision Trees
                    </a>
</li>
<li>
<a href="#sec_additional">
                        Additional Content
                    </a>
</li>
</ul>
</div>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_whatisR" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="6">
What's Decision Tree
        </font>

Decision tree learning is a method commonly used in data mining. The goal is to create a model that predicts the value of a target variable based on several input variables.
A decision tree is a simple representation for classifying examples. For this section, assume that all of the input features have finite discrete domains, and there is a single target feature called the "classification". Each element of the domain of the classification is called a class. A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature. The arcs coming from a node labeled with an input feature are labeled with each of the possible values of the target feature or the arc leads to a subordinate decision node on a different input feature. Each leaf of the tree is labeled with a class or a probability distribution over the classes, signifying that the data set has been classified by the tree into either a specific class, or into a particular probability distribution(which,if the decision tree is well-constructed, is skewed towards certain subsets of classes).
    </font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h2>Overview</h2>
<br/>
<img src="/notes/assets/pictures/overview.png"/>
<br/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><p></p>
<br/></p>
<div id="sec_installR" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="6">
Decision Tree Examples
        </font>
<br/><br/>
        1. Accept A New Job Offer
        <br/>
<img src="/notes/assets/pictures/find-job.png"/>
<br/>
        2. Predict Fuel Efficiency 
        <br/>
<img src="/notes/assets/pictures/fuel-efficiency.png"/>
<br/><br/>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/><div id="sec_R_packages" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="6">
Building A Decision Tree
        </font>
<br/><br/>
        Now we want to build a decision tree for Fuel Efficiency Example
        <img src="/notes/assets/pictures/fuel-efficiency2.png"/>
<h2>Build decision Tree</h2>
<h3>The Starting Node</h3>
        We can start with an empty tree and improve it at each level. For example, for the second example we have a tree that for all data, it returns mpg=bad. So for our dataset we got 22 correct answers and 18 wrong answers with this tree. but we can improve it!
        <img src="/notes/assets/pictures/fuel-efficiency3.png" style="width:200px;height:100px"/></font></div></p>
<p>&lt;/div&gt;</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div style="line-height:300%;">
<font face="XB Zar" size="5">
<h2>Operators</h2>
        There are several operators for building a decision tree.
        <h4>Improving the tree</h4>
        To improve our tree, we can add more nodes(features) to get more correct answer and predict better.
        <img src="/notes/assets/pictures/fuel-efficiency4.png"/>
<h4>Recursive step</h4>
        We can consider each leaf as a root and do the same for those.Maybe we get to some node that doesnâ€™t have any data. In these situations we should end up and predict randomly.
        <img src="/notes/assets/pictures/fuel-efficiency5.png">
        After one level we got this tree ?
        <img src="/notes/assets/pictures/fuel-efficiency6.png"/>
        And after adding all nodes(features) to the tree we got a Full Tree like this:
        <img src="/notes/assets/pictures/fuel-efficiency7.png"/>
</img></font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="sec_R_Intro" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="7">
        Two Questions For More Efficiency
        </font>
<p></p>
<hr/>
<h3>Hill Climbing Algorithm:</h3>

      â€¢Start from empty decision tree
      â€¢Split on the best attribute (feature) â€“ Recurse
 Now the important question is:

      â€¢Which attribute gives the best split? 
      â€¢When to stop recursion? 

</font></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="sec_R_Intro" style="line-height:300%;">
<font face="XB Zar" size="5">
<h3>Splitting: choosing a good attribute</h3>

Look at the example to get the main point of splitting! 
<img src="/notes/assets/pictures/p10.png"/>
<h4>Measuring uncertainty</h4>
<br/>
â€¢Good split if we are more certain about classification after split<br/>
Â Â Â Â Â â€¢Deterministic good (all true or all false)<br/>
Â Â Â Â Â â€¢Uniform distribution?   BAD<br/>
</font></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="sec_whatisR" style="line-height:300%;">
<font face="XB Zar" size="5">
<h3>Which attribute gives the best split? </h3>
        â€¢ A1: The one with the highest information gain<br/>
 Â Â Â Â Â Â â€¢ Defined in terms of entropy<br/>
 â€¢ A2: Actually many alternatives,<br/>
Â Â Â Â Â Â â€¢ e.g., accuracy. Seeks to reduce the misclassification rate

</font></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_R_Intro" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="7">
        Entropy And Information Gain
        </font>
<p></p>
<hr/>
<font color="purple" size="6">Entropy</font><br/>
        First we start with definition of entropy:<br/>
Â Â Â    Entropy H(Y) of a random variable Y:
<img src="/notes/assets/pictures/p16.png" style="width:400px"/>
As it can be figured out, more uncertainty, more entropy!<br/>
It's interesting to know that in Information Theory interpretation entropy is defined as:<br/>
Â Â Â Â Expected number of bits needed to encode a randomly drawn value of Y (under most efficient code)<br/>
And for a diagram view of entropy we can say it has a peak in the middle of it, then it falls.
<img src="/notes/assets/pictures/p17.png" style="height:300px"/>
<font color="green" size="5.7">Example of Entropy</font><br/>
If P(Y=t) = 5/6 and P(Y=f) = 1/6 the value of H(Y) would be:
<img src="/notes/assets/pictures/p18.png" style="height:300px"/>
<font color="purple" size="6">Conditional Entropy</font><br/>
definition:<br/>
Â Â Â Conditional Entropy H(Y|X) of a random variable Y conditioned on a random
variable X<br/>
<img src="/notes/assets/pictures/p19.png"/>
And here an example of conditional entropy:<br/>
<img src="/notes/assets/pictures/p20.png" style="height:300px"/>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="purple" size="6.6">Information Gain</font><br/>
        definition:<br/>
Â Â Â Â The information gain is the amount of information gained about a random variable or signal from observing another random variable and defined by this equation:
<img src="/notes/assets/pictures/p21.png"/>
And for IG of our last example we have:
<img src="/notes/assets/pictures/p22.png"/>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_statFunc" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="6">
        Learning Decision Tree
        </font><br/>
        For learning a decision tree we should consider some important things such as:<br/>
Â Â Â Â Â Â . Select a node for splitting<br/>
Â Â Â Â Â Â . Update tree after an iteration<br/>
Â Â Â Â Â Â . Terminating condition<br/>

Steps of building a tree:<br/>
Â Â Â Â Â Â . Start from empty decision tree<br/>
Â Â Â Â Â Â . Split on next best attribute (feature)<br/>
Â Â Â Â Â Â . Use information gain (or...?) to select attribute:<br/>
<img src="/notes/assets/pictures/p23.png" style="width:400px"/>
Â Â Â Â Â Â . Recurse<br/><br/>
<font color="green" size="6">Example</font><br/>       
Now we want to build a tree step by step, as an example for figuring out the way to build a DT.<br/>
Suppose we want to predict MPG; now look at all the information gains in the picture below
<img src="/notes/assets/pictures/p24.png"/>
Now we should do our iterations.<br/>
For first iteration we have:
<img src="/notes/assets/pictures/p25.png"/>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_statFunc" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="purple" size="6">
        Termination Conditions
        </font><br/>
        We introduce conditions to know when to stop building process.<br/>
        Two ideas for terminating are:<br/>
Â Â Â Â Â Â . Base Case One: If all records in current data subset have the same output then
donâ€™t recurse.<br/>
Â Â Â Â Â Â . Base Case Two: If all records have exactly the same set of input attributes then
donâ€™t recurse.<br/>
Base Case One:
        <img src="/notes/assets/pictures/p26.png"/>
Base Case Two:
        <img src="/notes/assets/pictures/p27.png"/>
<br/>
Now there is one other idea for termination that we want to discuss:<br/>
Â Â Â Â . Base Case 3: If all attributes have zero information gain then don't recurse.<br/>
<font color="red" size="5">
        Is it a good idea??
        </font><br/>
Â Â Â Â The answer is no!!<br/>
Because it's kind of a greedy idea!!<br/>
In this idea we check information gain of our variables alone, but if we check the information we get from combination of variables, there maybe be usefull information.<br/>
Here an example of this idea's problem:<br/>
Â Â Â Â For Y = a xor b, we use this idea and here the result:
<img src="/notes/assets/pictures/p28.png"/>
But without this idea:
<img src="/notes/assets/pictures/p29.png"/>
<img src="/notes/assets/pictures/p30.png"/>
<img src="/notes/assets/pictures/p31.png"/>
So as we can see, this idea didn't come up with good results.
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_R_DSs" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="6">
            Overfitting in Decision Trees
        </font>
<h4>How to resolve Overfitting ?</h4>

Decision trees have no learning bias and in outcome, the variance over the training set is significantly high. How can we introduce some biases to resolve the overfitting?
        <h5>No Free Lunch Theorem</h5>
Take the facing graph as an example. Suppose we are given a dataset of (x, y)s and we want to estimate the function which caused these points.
As you mightâ€™ve gotten it so far, all the three functions (red, green, and blue) are estimating it right and if we donâ€™t have any prior knowledge, all three functions have the exact probability to be our suggesting function.
â€œNo Free lunchâ€ states that, in absence of any sense on what functions are more likely, learning is impossible.
For example, if we already knew that a very smooth function generated these points we are more likely to prefer the blue function to others.
<img src="/notes/assets/pictures/p11.png" style="width:400px;height:300px"/>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_whatisR" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#000000" size="5">
<b>
<font color="gray" size="6">
                Occamâ€™s Razor :
                    </font>
</b>
</font>
<br/>
        This principle states that from all possible hypotheses on a dataset, choose the shortest (the least complex) one. Because a short hypothesis is less likely to overfit.
        <img src="/notes/assets/pictures/p12.png" style="width:700px;height:300px"/>
<h6>Variance of model</h6>
        If we change the training data, how much our result on validation set changes
        <h6>Bias of model</h6>
        The deviation we took from the original training data while we were training
        <br/>
        According to this principle weâ€™d rather make smaller trees with less depth.

</font></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="sec_whatisR" style="line-height:300%;">
<font face="XB Zar" size="5">
<b>
<font color="brown" size="6.5">How To Build Small Trees</font>
</b>
<br/>
        There are some approaches as followed:
        <br/>
<br/>
<font color="purple" size="6">Stop growing tree before overfit<br/></font>
Â Â Â Â Â Â . Bound depth or #leaves<br/>
Â Â Â Â Â Â . Stop growing more depth if in one depth Information Gain (IG) of all<br/>
Â Â Â Â Â Â . features on remaining data is zero.<br/>
<font color="purple" size="6">Grow full tree then prune<br/></font>
Â Â Â Â Â Â . Optimize on a held-out (validation set). Meaning after the training is finished prune any branches that do not reduce the accuracy on validation set below a threshold. (Con: requires a larger amount of data)<br/>
Â Â Â Â Â Â . Use statistical significance testing<br/>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div style="line-height:300%;">
<font face="XB Zar" size="5">
<b>
<font color="brown" size="6.5">Use statistical significance testing</font>
</b>
<font color="purple" size="6">Chi-Square Test (remembrance)<br/></font>
Having two features where the first feature has k classes and the second one has n classes. With people which we know their classes in both features, H0(null hypothesis) states that in a table with k rows and n columns, rows are independent from columns and conversely.
        <br/>
<img src="/notes/assets/pictures/p13.png"/>
<font color="purple" size="6">Using Chi-Square in growing tree<br/></font>
As you know, for choosing what feature to be used in each depth we got help from a concept called IG. after the tree is grown (fully or with a limit on depth and leaves) we test each feature in each depth and calculate the p_value. After setting a threshold calledMaxPchance, if calculate pfor any feature is greater than MaxPchance. We prune that feature and the decision will be based on the previous feature (parent node).
How to find a good MaxPchance?
We could use a local or greedy search on the validation set and by reaching a good accuracy on the validation set we would stop and set that MaxPchance.


</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_additional" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="7">
Additional Content<br/>
</font>
<font color="purple" size="6.5">Random Forests<br/></font>
Idea:<br/> 
Â Â Â Â Â Decision Trees overfit easily and it's hard to grow them on a big number of features.<br/>
Approach:<br/> 
Â Â Â Â Â Use multiple decision trees instead of only one.
<img src="/notes/assets/pictures/p14.png"/>
For each tree:<br/>
<font color="green" size="6">Training Set</font><br/>
Assume T as the original training set. Itâ€™s not usual to divide it into N subsets (N number of decision trees in forest). For  the sake of resolving the overfit, we choose a subset size of $Ï†1|T|$ then we bootstrap (bagging) T into N subsets of size $Ï†1|T|$. ($0 &lt; Ï†1|T| &lt;=1$)<br/>
<font color="green" size="6">Features</font><br/>
One Problem of a single decision tree was that it was hard to grow it on a lot of features. Then here we choose randomly a subset of features with size $Ï†2|F|$ and assign it to each tree in the forest. ($0 &lt; Ï†2|F| &lt;=1$)<br/>
<font color="green" size="6">Prediction</font><br/>
There are two methods to report the final prediction of each sample of the test set.<br/>
<font color="purple" size="5.5">1. Max Method</font><br/>
In this method we report the class which is predicted the most. for example if 3 out of 5 trees said the class is  ğ‘Œ=1 we report  ğ‘Œ=1 . properties:<br/>
Â Â Â Â Â Â . less F1 score<br/>
Â Â Â Â Â Â . more recall<br/>
Â Â Â Â Â Â . less precision<br/>
Â Â Â Â Â Â . should choose randomly when the number of trees is even<br/>
<font color="purple" size="5.5">2. Avg Method</font><br/>
As we know each tree has a probability with its prediction which shows how much does he persist that its answer is true. We calculate a weighted average of each tree's prediction where the weights are the probability reported from each tree alongside its prediction.<br/> properties:<br/>
Â Â Â Â Â Â . more F1<br/>
Â Â Â Â Â Â . less recall<br/>
Â Â Â Â Â Â . more precision<br/>
Â Â Â Â Â Â . has a more reasonable answer when the number of trees is even<br/>
<img src="/notes/assets/pictures/p15.png"/>
</font>
</div>
</div>
</div>
</div>
